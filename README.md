# DGKD
This is the code for the paper "Decoupled Graph Knowledge Distillation: A General Logits-Based Method for Learning MLPs on Graphs."

DGKD.py is the core loss function in this paper, which is a plug-and-play encapsulated module. You can use it in any graph knowledge distillation framework to further improve the prediction performance of student MLP.

We will release the complete source code later.
